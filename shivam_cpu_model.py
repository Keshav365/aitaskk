# -*- coding: utf-8 -*-
"""Shivam CPU Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BdK3E9TDG6YV5YagxL_BXHK86VPjljeI
"""

!pip install ultralytics filterpy supervision

!wget https://raw.githubusercontent.com/abewley/sort/master/sort.py

import matplotlib.pyplot as plt
import cv2
import torch
import numpy as np
from ultralytics import YOLO
import supervision as sv
import subprocess
from IPython.display import Video

import sys
sys.path.append('/content/sort')
import sort
print(dir(sort))
from sort import Sort

class CFG:
    MODEL_WEIGHTS = 'yolov8n.pt'
    CONFIDENCE = 0.35
    IOU = 0.5
    HEATMAP_ALPHA = 0.30
    RADIUS = 20
    TRACK_SECONDS = 5
    TRACK_THRESH = 0.35
    MATCH_THRESH = 0.9999
    DETECTION_VIDEO = "/content/drive/MyDrive/YoloV3Tiny/sample.mp4"
    HEATMAP_VIDEO = "/content/drive/MyDrive/YoloV3Tiny/cctv.mp4"
    OUTPUT_PATH = './'


model = YOLO(CFG.MODEL_WEIGHTS)
tracker = Sort()

def get_video_properties(video_path):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError("Could not open video file")
    properties = {
        "fps": int(cap.get(cv2.CAP_PROP_FPS)),
        "frame_count": int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
        "duration_seconds": int(cap.get(cv2.CAP_PROP_FRAME_COUNT) / cap.get(cv2.CAP_PROP_FPS)),
        "width": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
        "height": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
        "codec": int(cap.get(cv2.CAP_PROP_FOURCC)),
    }
    cap.release()
    return properties

detection_video_properties = get_video_properties(CFG.DETECTION_VIDEO)
heatmap_video_properties = get_video_properties(CFG.HEATMAP_VIDEO)

cap = cv2.VideoCapture(CFG.DETECTION_VIDEO)
out = cv2.VideoWriter(f"{CFG.OUTPUT_PATH}detection_output.mp4", cv2.VideoWriter_fourcc(*'mp4v'), 30, (int(cap.get(3)), int(cap.get(4))))

heat_map_annotator = sv.HeatMapAnnotator(
    position=sv.Position.BOTTOM_CENTER,
    opacity=CFG.HEATMAP_ALPHA,
    radius=CFG.RADIUS,
    kernel_size=25,
    top_hue=0,
    low_hue=125,
)

label_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)

byte_tracker = sv.ByteTrack(
    track_activation_threshold=CFG.TRACK_THRESH,
    lost_track_buffer=CFG.TRACK_SECONDS * detection_video_properties['fps'],
    minimum_matching_threshold=CFG.MATCH_THRESH,
    frame_rate=detection_video_properties['fps']
)

class_labels = {}
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    results = model(frame)
    detections = []
    for result in results:
        for box in result.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            conf = float(box.conf[0])
            cls = int(box.cls[0])
            label = model.names[cls]
            if conf > CFG.CONFIDENCE:
                detections.append([x1, y1, x2, y2, conf])
    detections = np.array(detections)
    tracked_objects = tracker.update(detections) if len(detections) > 0 else []
    for obj in tracked_objects:
        x1, y1, x2, y2, obj_id = map(int, obj)
        if obj_id not in class_labels:
            class_labels[obj_id] = label
        label = class_labels[obj_id]
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        text = f"{obj_id}"
        cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
    out.write(frame)
cap.release()
out.release()

frames_generator = sv.get_video_frames_generator(source_path=CFG.HEATMAP_VIDEO, stride=1)
output_filename = f'{CFG.OUTPUT_PATH}heatmap_output.mp4'
with sv.VideoSink(target_path=output_filename, video_info=sv.VideoInfo.from_video_path(CFG.HEATMAP_VIDEO)) as sink:
    for frame in frames_generator:
        result = model(source=frame, classes=[0], conf=CFG.CONFIDENCE, iou=CFG.IOU, half=True, show_conf=True, save_txt=True, save_conf=True, save=True, device=[0,1])[0]
        detections = sv.Detections.from_ultralytics(result)
        detections = byte_tracker.update_with_detections(detections)
        annotated_frame = heat_map_annotator.annotate(scene=frame.copy(), detections=detections)
        labels = [f"#{tracker_id}" for class_id, tracker_id in zip(detections.class_id, detections.tracker_id)]
        label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)
        sink.write_frame(frame=annotated_frame)


subprocess.run(["ffmpeg", "-i", f"{CFG.OUTPUT_PATH}detection_output.mp4", "-crf", "18", "-preset", "veryfast", "-hide_banner", "-loglevel", "error", "-vcodec", "libx264", f"{CFG.OUTPUT_PATH}detection_output_new_codec.mp4"])
subprocess.run(["ffmpeg", "-i", f"{CFG.OUTPUT_PATH}heatmap_output.mp4", "-crf", "18", "-preset", "veryfast", "-hide_banner", "-loglevel", "error", "-vcodec", "libx264", f"{CFG.OUTPUT_PATH}heatmap_output_new_codec.mp4"])

# Display videos
Video(data=f"{CFG.OUTPUT_PATH}detection_output_new_codec.mp4", embed=True, height=int(detection_video_properties['height'] * 0.5), width=int(detection_video_properties['width'] * 0.5))
Video(data=f"{CFG.OUTPUT_PATH}heatmap_output_new_codec.mp4", embed=True, height=int(heatmap_video_properties['height'] * 0.5), width=int(heatmap_video_properties['width'] * 0.5))

import pandas as pd
file_path = f'{CFG.OUTPUT_PATH}runs/detect/predict/labels/image0.txt'

columns = ['class', 'x_center', 'y_center', 'width', 'height', 'confidence']
predictions = pd.read_csv(file_path, delimiter=' ', header=None, names=columns)


predictions['class_name'] = predictions['class'].map(model.names)
predictions = predictions[['class_name'] + columns]
predictions

